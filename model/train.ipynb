{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5134,
     "status": "ok",
     "timestamp": 1715714393995,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "AKi8OPICqMZh",
    "outputId": "6850f18e-debb-4a51-aa9f-815db0f87a1d"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2857,
     "status": "ok",
     "timestamp": 1715714396850,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "gm2pq14VqxUw",
    "outputId": "7529264d-026b-46fd-fcc2-c11d5486f5ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7585,
     "status": "ok",
     "timestamp": 1715714404433,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "FYJ5LmnuVi9P",
    "outputId": "8d54653a-ac57-4945-ba7c-2ed87d04220b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertModel, BertTokenizer\n",
    "#import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "#import wandb\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_UpYqhgvQDO"
   },
   "outputs": [],
   "source": [
    "#Setup CNN for reverse pass\n",
    "class smallBoy(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=128, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(c_in, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVrtfzj6qMZj"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, img_dir, caption_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        with open(caption_file, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "        self.filenames = list(self.captions.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        caption = self.captions[img_name]\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD95tYDzqMZk"
   },
   "outputs": [],
   "source": [
    "def apply_canny_edge_detector(image, low_threshold=100, high_threshold=200):\n",
    "    image = image * 255\n",
    "    batch_np = image.numpy().astype(np.uint8)\n",
    "    edge_maps = []\n",
    "\n",
    "    for image_np in batch_np:\n",
    "        if image_np.shape[0] == 3:\n",
    "            image_np = cv2.cvtColor(image_np.transpose(1, 2, 0), cv2.COLOR_RGB2GRAY)\n",
    "        blurred_image = cv2.GaussianBlur(image_np, (3, 3), 0)\n",
    "        edges = cv2.Canny(blurred_image, low_threshold, high_threshold)\n",
    "        edge_maps.append(edges)\n",
    "\n",
    "    edge_tensor = torch.from_numpy(np.stack(edge_maps, axis=0)).unsqueeze(1).float() / 255\n",
    "    return edge_tensor\n",
    "\n",
    "def apply_canny_edge_detector_rgb(image, low_threshold=100, high_threshold=200):\n",
    "  image = image * 255\n",
    "  batch_np = image.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n",
    "  edge_masked_images = []\n",
    "\n",
    "  for image_np in batch_np:\n",
    "      if image_np.shape[-1] == 3:\n",
    "          gray_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "      else:\n",
    "          gray_image = image_np.squeeze()\n",
    "\n",
    "      blurred_image = cv2.GaussianBlur(gray_image, (3, 3), 0)\n",
    "      edges = cv2.Canny(blurred_image, low_threshold, high_threshold)\n",
    "      edge_mask = edges / 255.0\n",
    "      edge_mask = np.expand_dims(edge_mask, axis=-1)\n",
    "      edge_masked_image = image_np * edge_mask\n",
    "      edge_masked_image[edge_masked_image == 0] = 127.5\n",
    "      edge_masked_images.append(edge_masked_image)\n",
    "\n",
    "  edge_masked_tensor = torch.from_numpy(np.stack(edge_masked_images, axis=0)).permute(0, 3, 1, 2).float() / 255\n",
    "  return edge_masked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qK6XXcTxqMZk"
   },
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, beta_start, beta_end, steps, device = \"cuda\"):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.delta_network = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.recovery_network = smallBoy(c_in=3, c_out=3, time_dim=256, device=self.device)\n",
    "\n",
    "        #Noise schedule\n",
    "        self.beta_t = torch.linspace(beta_start, beta_end, steps=steps)\n",
    "        self.alpha_t = torch.cumprod(1 - self.beta_t, dim=0)\n",
    "\n",
    "        #Text embeddings\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def compute_delta(self, text_features, t):\n",
    "        delta_scale = self.delta_network(text_features).squeeze()\n",
    "        return delta_scale\n",
    "\n",
    "    def forward_pass(self, m, m_rgb, captions, t):\n",
    "        inputs = self.bert_tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.bert_model(**inputs)\n",
    "        text_embed = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        alpha_t = self.alpha_t[t]\n",
    "        one_minus_alpha_t = 1 - alpha_t\n",
    "        unmasked_regions = torch.ones_like(m) - m\n",
    "        delta = self.compute_delta(text_embed, t).float()\n",
    "\n",
    "        delta = delta.float()\n",
    "        weighted_noise = torch.randn_like(m) * delta[0]\n",
    "\n",
    "        m_rgb = m_rgb.float()\n",
    "        x_t = m + (unmasked_regions)*(torch.sqrt(alpha_t) * m_rgb * unmasked_regions + torch.sqrt(one_minus_alpha_t) * weighted_noise)\n",
    "        return torch.clamp(x_t, 0, 1)\n",
    "\n",
    "    '''\n",
    "    def reverse_pass(self, x_t, m, t):\n",
    "        # Compute recovery parameters\n",
    "        t = torch.tensor(t, dtype=torch.long)\n",
    "        recovery_params = self.recovery_network(x_t, t)\n",
    "        unmasked_regions = torch.ones_like(m) - m\n",
    "\n",
    "        # Get noise\n",
    "        noise = torch.randn_like(x_t)\n",
    "        beta = self.beta_t[t]\n",
    "\n",
    "        # Reverse pass equation\n",
    "        x_0 = 1 / torch.sqrt(self.alpha_t[t]) * (x_t - unmasked_regions * (1-self.alpha_t[t])/(torch.sqrt(1 - self.alpha_t[t])) * recovery_params) + torch.sqrt(beta) * noise * unmasked_regions\n",
    "        return torch.clamp(x_0, 0, 1)\n",
    "    '''\n",
    "    def reverse_pass(self, x_t, m, t):\n",
    "        unmasked_regions = 1 - m\n",
    "        for i in reversed(range(t)):\n",
    "            t_i = torch.tensor(i, dtype=torch.long).to(x_t.device)\n",
    "            recovery_params = self.recovery_network(x_t, t_i)\n",
    "\n",
    "            noise = torch.randn_like(x_t, device=x_t.device)\n",
    "            beta = self.beta_t[i]\n",
    "\n",
    "            #Calculate reusable values\n",
    "            alpha_t_sqrt_inv = 1 / torch.sqrt(self.alpha_t[i])\n",
    "            alpha_t_sqrt_diff_inv = (1 - self.alpha_t[i]) / torch.sqrt(1 - self.alpha_t[i])\n",
    "            beta_sqrt = torch.sqrt(beta)\n",
    "\n",
    "            #Reverse pass equation\n",
    "            x_tminus1 = (\n",
    "                alpha_t_sqrt_inv * (x_t - unmasked_regions * alpha_t_sqrt_diff_inv * recovery_params) +\n",
    "                beta_sqrt * noise * unmasked_regions\n",
    "            )\n",
    "\n",
    "            x_t = torch.clamp(x_tminus1, 0, 1)\n",
    "\n",
    "            del t_i, recovery_params, noise, beta, alpha_t_sqrt_inv, alpha_t_sqrt_diff_inv, beta_sqrt, x_tminus1\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def diffusion_loss(self, actual_noise, predicted_noise):\n",
    "      loss = F.mse_loss(predicted_noise, actual_noise)\n",
    "      return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvLiw4iF8jIc"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.detach()\n",
    "    if inp.is_cuda:\n",
    "        inp = inp.cpu()\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "def show_images(images, title=\"\"):\n",
    "    out = make_grid(images)\n",
    "    imshow(out, title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbJUnZRVj0Vy"
   },
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "  checkpoint_files = sorted(os.listdir(checkpoint_dir), key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)))\n",
    "  latest_checkpoint = checkpoint_files[-1] if checkpoint_files else None\n",
    "  if latest_checkpoint:\n",
    "    latest_checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "\n",
    "    model = DiffusionModel(beta_start=0.0001, beta_end=0.02, steps=1000, device=\"cuda\")\n",
    "    model.load_state_dict(torch.load(latest_checkpoint_path))\n",
    "\n",
    "    print(f\"Loaded checkpoint '{latest_checkpoint}'\")\n",
    "    return model\n",
    "  else:\n",
    "      print(\"No checkpoints found.\")\n",
    "      model = DiffusionModel(beta_start=0.0001, beta_end=0.02, steps=1000, device=\"cuda\")\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1715714572937,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "lIpbBzmFjow6",
    "outputId": "30677320-8e63-407b-e743-3c5e268e2676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found.\n"
     ]
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 1e-4\n",
    "\n",
    "config = Params()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.CenterCrop((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "T = 1000 #Total number of diffusion steps\n",
    "\n",
    "#wandb.init(project='my_diffusion_project', entity='michaelpeng72', config=config, id='diffusion_training', resume = 'allow')\n",
    "\n",
    "dataset_path = '/content/drive/My Drive/Final Project/datasets'\n",
    "img_dir = os.path.join(dataset_path, 'images')\n",
    "caption_file = os.path.join(dataset_path, 'map.json')\n",
    "full_dataset = ImageCaptionDataset(img_dir, caption_file, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "valid_size = len(full_dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "model = load_latest_checkpoint('/content/drive/My Drive/Final Project/checkpoints/second_model')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 111669,
     "status": "error",
     "timestamp": 1715714715835,
     "user": {
      "displayName": "Michael Peng",
      "userId": "09920448190389534354"
     },
     "user_tz": 240
    },
    "id": "VUpEknK0qMZl",
    "outputId": "7e941a1a-3fbc-47f8-bae0-ca395f7844f7"
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, config.epochs):\n",
    "    print(\"Starting Epoch: \", epoch)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_bits = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    for images, captions in train_loader:\n",
    "        m_rgb = apply_canny_edge_detector_rgb(images)\n",
    "        m = apply_canny_edge_detector(images)\n",
    "        t = torch.randint(0, T, (1,)).item()\n",
    "\n",
    "        show_images(images[0])\n",
    "        show_images(m_rgb[0])\n",
    "\n",
    "        noised_images = model.forward_pass(m, m_rgb, captions, t)\n",
    "        recovered_images = model.reverse_pass(noised_images, m, t)\n",
    "\n",
    "        show_images(recovered_images[0])\n",
    "\n",
    "        loss = model.diffusion_loss((noised_images - images), (noised_images - recovered_images))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #wandb.log({\"train_loss_by_batch\": loss.item()})\n",
    "        total_bits += m.sum().item()\n",
    "        total_pixels += images.numel() / images.shape[1]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    bpp = total_bits / total_pixels\n",
    "    #wandb.log({\"train_loss_by_epoch\": train_loss})\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in valid_loader:\n",
    "            m_rgb = apply_canny_edge_detector_rgb(images)\n",
    "            m = apply_canny_edge_detector(images)\n",
    "            t = torch.randint(0, T, (1,)).item()\n",
    "\n",
    "            noised_images = model.forward_pass(m, m_rgb, captions, t)\n",
    "            recovered_images = model.reverse_pass(noised_images, m, t)\n",
    "\n",
    "            loss = model.diffusion_loss((noised_images - images), (noised_images - recovered_images))\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "    #wandb.log({\"valid_loss\": valid_loss})\n",
    "\n",
    "    compression_rate = np.mean(compression_rate)\n",
    "    #wandb.log({\"compression_rate\": compression_rate})\n",
    "\n",
    "    print(\"Epoch: \", epoch, \"Train Loss: \", train_loss, \"Valid Loss: \", valid_loss, \"Compression Rate: \", bpp)\n",
    "\n",
    "    checkpoint_path = f'/content/drive/My Drive/Final Project/checkpoints/epoch_{epoch}_checkpoint.pth'\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVW9LcF5iWro"
   },
   "source": [
    "Benchmark Inference Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq7qkaCHqMZl"
   },
   "outputs": [],
   "source": [
    "model = load_latest_checkpoint('/content/drive/My Drive/2023-24/6.8300/Final Project/checkpoints/first_model')\n",
    "config.batch_size = 1\n",
    "def measure_inference_speed(model, loader, batch_sizes, T):\n",
    "    times = {bs: [] for bs in batch_sizes}\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        print(bs)\n",
    "        model.eval()\n",
    "        batch_data = []\n",
    "        images_processed = 0\n",
    "        for images, captions in loader:\n",
    "            if len(batch_data) < bs:\n",
    "                batch_data.append((images, captions))\n",
    "            if len(batch_data) >= bs:\n",
    "                images = torch.cat([data[0] for data in batch_data], dim=0)\n",
    "                captions = [caption for data in batch_data for caption in data[1]]\n",
    "                batch_data = []\n",
    "\n",
    "                m_rgb = apply_canny_edge_detector_rgb(images)\n",
    "                m = apply_canny_edge_detector(images)\n",
    "                t = torch.randint(0, T, (1,)).item()\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                noised_images = model.forward_pass(m, m_rgb, captions, t)\n",
    "                recovered_images = model.reverse_pass(noised_images, m, t)\n",
    "\n",
    "                end_time = time.time()\n",
    "                times[bs].append(end_time - start_time)\n",
    "\n",
    "\n",
    "                break\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        if times[bs]: #avoid division by zero\n",
    "            print(f'Average inference time for batch size {bs}: {sum(times[bs]) / len(times[bs]):.4f} seconds')\n",
    "        else:\n",
    "            print(f'No data for batch size {bs}')\n",
    "\n",
    "batch_sizes = [1, 5, 25, 50, 100]\n",
    "measure_inference_speed(model, valid_loader, batch_sizes, T)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
