{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision transformers\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, img_dir, seg_dir, caption_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.transform = transform\n",
    "        with open(caption_file, 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "        self.filenames = list(self.captions.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        seg_name = 'segmented_' + img_name  #Adjusting name for segmented images\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        seg_path = os.path.join(self.seg_dir, seg_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        segmentation = Image.open(seg_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            segmentation = self.transform(segmentation)\n",
    "\n",
    "        caption = self.captions[img_name]\n",
    "        return image, segmentation, caption\n",
    "\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_sobel_operator(segmentation):\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "    \n",
    "    # Ensure the segmentation tensor is float and has a batch dimension\n",
    "    if len(segmentation.shape) == 2:\n",
    "        segmentation = segmentation.unsqueeze(0).unsqueeze(0)\n",
    "    elif len(segmentation.shape) == 3:\n",
    "        segmentation = segmentation.unsqueeze(1)\n",
    "\n",
    "    edges_x = F.conv2d(segmentation, sobel_x, padding=1)\n",
    "    edges_y = F.conv2d(segmentation, sobel_y, padding=1)\n",
    "\n",
    "    edges = torch.sqrt(edges_x**2 + edges_y**2)\n",
    "\n",
    "    threshold = edges.mean() * 1.5\n",
    "    binary_mask = (edges > threshold).float()\n",
    "\n",
    "    return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mu_network = nn.Sequential(\n",
    "            nn.Conv2d(128, 3, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # Network to predict dynamic beta values (noise levels)\n",
    "        self.beta_network = nn.Sequential(\n",
    "            nn.Linear(768, 100),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, m, captions, t):\n",
    "        # Encode text\n",
    "        inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        text_features = self.text_encoder(**inputs).pooler_output\n",
    "\n",
    "        # Get dynamic noise level (beta_t) from text features\n",
    "        beta_schedule = self.beta_network(text_features)\n",
    "        beta_t = beta_schedule[:, t].unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Apply noise where m is 0 (non-segmentation areas)\n",
    "        noise = torch.randn_like(x_t) * torch.sqrt(beta_t)\n",
    "        x_t1_pred = torch.sqrt(1 - beta_t) * x_t + (1 - m) * noise\n",
    "\n",
    "        return x_t1_pred\n",
    "\n",
    "model = DiffusionModel()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss(x_t, x_t1_pred, m):\n",
    "    return ((x_t1_pred - x_t) ** 2 * (1 - m)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "T = 1000  #Total number of diffusion steps\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "\n",
    "# Linear schedule\n",
    "beta_t = torch.linspace(beta_start, beta_end, steps=T)\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project='image_captioning_with_diffusion_models', entity='michaelpeng72', config={\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"n_splits\": n_splits,\n",
    "    \"shuffle\": True,\n",
    "    \"random_state\": 42\n",
    "})\n",
    "\n",
    "# Config is accessible via wandb.config\n",
    "config = wandb.config\n",
    "\n",
    "full_dataset = ImageCaptionDataset('datasets/images', 'datasets/segmented', 'datasets/map.json', transform=transform)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(full_dataset)):\n",
    "    print(f\"Starting Fold {fold+1}/{n_splits}\")\n",
    "    wandb.init(project='image_captioning_with_diffusion_models', entity='michaelpeng72',\n",
    "               group=\"Experiment-X\", job_type=f\"Fold-{fold+1}\", reinit=True)\n",
    "\n",
    "    # Splitting the dataset into train and validation for the current fold\n",
    "    train_subset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "    valid_subset = torch.utils.data.Subset(full_dataset, valid_idx)\n",
    "\n",
    "    # Create DataLoader for train and validation subsets\n",
    "    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    model = DiffusionModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for images, segmentations, captions in train_loader:\n",
    "            m = apply_sobel_operator(segmentations)\n",
    "            t = torch.randint(0, 100, (1,)).item()\n",
    "\n",
    "            # Forward pass\n",
    "            x_t1_pred = model(images, m, captions, t)\n",
    "            loss = diffusion_loss(images, x_t1_pred, m)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        wandb.log({\"Train Loss\": train_loss})\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval() \n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, segmentations, captions in valid_loader:\n",
    "                m = (segmentations > 0.5).float()\n",
    "                t = torch.randint(0, 100, (1,)).item()\n",
    "                \n",
    "                x_t1_pred = model(images, m, captions, t)\n",
    "                loss = diffusion_loss(images, x_t1_pred, m)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        valid_loss /= len(valid_loader)\n",
    "\n",
    "        wandb.log({\"Validation Loss\": valid_loss})\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "        \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model to ../models\n",
    "torch.save(model.state_dict(), 'models/diffusion_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from ../models\n",
    "model = DiffusionModel()\n",
    "model.load_state_dict(torch.load('models/diffusion_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
