{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AKi8OPICqMZh","outputId":"928a5052-f8ff-4370-fe3f-aace084e4c6a","executionInfo":{"status":"ok","timestamp":1715700914342,"user_tz":240,"elapsed":103739,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install torch torchvision transformers\n","#!pip install wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gm2pq14VqxUw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715701171483,"user_tz":240,"elapsed":257150,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"}},"outputId":"1be6c9f5-0dcc-40d6-ecdd-008fa91e89b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15535,"status":"ok","timestamp":1715701187015,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"},"user_tz":240},"id":"FYJ5LmnuVi9P","outputId":"aed27de3-6921-472d-831c-f3d018cbbb68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","import cv2\n","import numpy as np\n","import torch.nn as nn\n","import os\n","import json\n","import time\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from transformers import BertModel, BertTokenizer\n","#import wandb\n","from sklearn.model_selection import KFold\n","import matplotlib.pyplot as plt\n","from torchvision.utils import make_grid\n","#import wandb\n","from torchvision import transforms\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1715701187015,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"},"user_tz":240},"id":"D_UpYqhgvQDO"},"outputs":[],"source":["#Setup CNN for reverse pass\n","class smallBoy(nn.Module):\n","    def __init__(self, c_in=3, c_out=3, time_dim=128, device=\"cpu\"):\n","        super().__init__()\n","        self.device = device\n","        self.time_dim = time_dim\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(c_in, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x, t):\n","        return self.cnn(x)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715701187015,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"},"user_tz":240},"id":"LVrtfzj6qMZj"},"outputs":[],"source":["#Setup Dataloader\n","class ImageCaptionDataset(Dataset):\n","    def __init__(self, img_dir, caption_file, transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        with open(caption_file, 'r') as f:\n","            self.captions = json.load(f)\n","        self.filenames = list(self.captions.keys())\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.filenames[idx]\n","        img_path = os.path.join(self.img_dir, img_name)\n","\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        caption = self.captions[img_name]\n","        return image, caption"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1715701187015,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"},"user_tz":240},"id":"gD95tYDzqMZk"},"outputs":[],"source":["#Edge detection using canny\n","def apply_canny_edge_detector(image, low_threshold=100, high_threshold=200):\n","    image = image * 255\n","    batch_np = image.numpy().astype(np.uint8)\n","    edge_maps = []\n","\n","    for image_np in batch_np:\n","        if image_np.shape[0] == 3:\n","            image_np = cv2.cvtColor(image_np.transpose(1, 2, 0), cv2.COLOR_RGB2GRAY)\n","        blurred_image = cv2.GaussianBlur(image_np, (3, 3), 0)\n","        edges = cv2.Canny(blurred_image, low_threshold, high_threshold)\n","        edge_maps.append(edges)\n","\n","    edge_tensor = torch.from_numpy(np.stack(edge_maps, axis=0)).unsqueeze(1).float() / 255\n","    return edge_tensor\n","\n","#canny for rgb images\n","def apply_canny_edge_detector_rgb(image, low_threshold=100, high_threshold=200):\n","  image = image * 255\n","  batch_np = image.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n","  edge_masked_images = []\n","\n","  for image_np in batch_np:\n","      if image_np.shape[-1] == 3:\n","          gray_image = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n","      else:\n","          gray_image = image_np.squeeze()\n","\n","      blurred_image = cv2.GaussianBlur(gray_image, (3, 3), 0)\n","      edges = cv2.Canny(blurred_image, low_threshold, high_threshold)\n","      edge_mask = edges / 255.0\n","      edge_mask = np.expand_dims(edge_mask, axis=-1)\n","      edge_masked_image = image_np * edge_mask\n","      edge_masked_image[edge_masked_image == 0] = 127.5\n","      edge_masked_images.append(edge_masked_image)\n","\n","  edge_masked_tensor = torch.from_numpy(np.stack(edge_masked_images, axis=0)).permute(0, 3, 1, 2).float() / 255\n","  return edge_masked_tensor"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":194,"status":"ok","timestamp":1715701673468,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"},"user_tz":240},"id":"qK6XXcTxqMZk"},"outputs":[],"source":["#Diffusion Model\n","class DiffusionModel(nn.Module):\n","    def __init__(self, beta_start, beta_end, steps, device = \"cuda\"):\n","        super(DiffusionModel, self).__init__()\n","        self.device = device\n","\n","        # Networks definition\n","        self.delta_network = nn.Sequential(\n","            nn.Linear(768, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 1)\n","        )\n","        self.recovery_network = smallBoy(c_in=3, c_out=3, time_dim=256, device=self.device)\n","\n","        # Noise schedule\n","        self.beta_t = torch.linspace(beta_start, beta_end, steps=steps)\n","        self.alpha_t = torch.cumprod(1 - self.beta_t, dim=0)\n","\n","        # Text embeddings\n","        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n","        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def compute_delta(self, text_features, t):\n","        delta_scale = self.delta_network(text_features).squeeze()\n","        return delta_scale\n","\n","    def forward_pass(self, m, m_rgb, captions, t):\n","        # Get text embeddings using BERT\n","        inputs = self.bert_tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n","        outputs = self.bert_model(**inputs)\n","        text_embed = outputs.last_hidden_state[:, 0, :]\n","\n","        # Forward pass parameters\n","        alpha_t = self.alpha_t[t]\n","        one_minus_alpha_t = 1 - alpha_t\n","        unmasked_regions = torch.ones_like(m) - m\n","        delta = self.compute_delta(text_embed, t).float()\n","\n","        # Generate noise\n","        delta = delta.float()\n","        weighted_noise = torch.randn_like(m) * delta[0]\n","\n","        # Forward pass equation\n","        m_rgb = m_rgb.float()\n","        x_t = m + (unmasked_regions)*(torch.sqrt(alpha_t) * m_rgb * unmasked_regions + torch.sqrt(one_minus_alpha_t) * weighted_noise)\n","        return torch.clamp(x_t, 0, 1)\n","\n","    def reverse_pass(self, x_t, m, t):\n","        # Compute recovery parameters\n","        t = torch.tensor(t, dtype=torch.long)\n","        recovery_params = self.recovery_network(x_t, t)\n","        unmasked_regions = torch.ones_like(m) - m\n","\n","        # Get noise\n","        noise = torch.randn_like(x_t)\n","        beta = self.beta_t[t]\n","\n","        # Reverse pass equation\n","        x_0 = 1 / torch.sqrt(self.alpha_t[t]) * (x_t - unmasked_regions * (1-self.alpha_t[t])/(torch.sqrt(1 - self.alpha_t[t])) * recovery_params) + torch.sqrt(beta) * noise * unmasked_regions\n","        return torch.clamp(x_0, 0, 1)\n","\n","    def reverse_pass(self, x_t, m, t):\n","      \"\"\"\n","      Performs the reverse pass to denoise the image multiple times.\n","\n","      Args:\n","          x_t (torch.Tensor): The noisy input tensor.\n","          m (torch.Tensor): The mask tensor.\n","          t (int): The number of reverse diffusion steps.\n","\n","      Returns:\n","          torch.Tensor: The denoised output tensor.\n","      \"\"\"\n","      unmasked_regions = torch.ones_like(m) - m\n","      for i in reversed(range(t)):\n","          t_1 = torch.tensor(i, dtype=torch.long)\n","          recovery_params = self.recovery_network(x_t, t_i)\n","\n","          # Get noise\n","          noise = torch.randn_like(x_t)\n","          beta = self.beta_t[t_i]\n","\n","          # Reverse pass equation\n","          x_tminus1 = (1 / torch.sqrt(self.alpha_t[t]) *\n","                (x_t - unmasked_regions * (1 - self.alpha_t[t]) / torch.sqrt(1 - self.alpha_t[t]) * recovery_params) +\n","                torch.sqrt(beta) * noise * unmasked_regions)\n","\n","          x_t = torch.clamp(x_tminus1, 0, 1)\n","\n","      return x_t\n","\n","    def diffusion_loss(self, actual_noise, predicted_noise):\n","      loss = F.mse_loss(predicted_noise, actual_noise)\n","      return loss\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":501,"status":"ok","timestamp":1715701676156,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"},"user_tz":240},"id":"TvLiw4iF8jIc"},"outputs":[],"source":["#Helper function to plot images\n","def imshow(inp, title=None):\n","    #Ensure that input is on CPU\n","    inp = inp.detach()\n","    if inp.is_cuda:\n","        inp = inp.cpu()\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)\n","\n","def show_images(images, title=\"\"):\n","    out = make_grid(images)\n","    imshow(out, title=title)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1715701677002,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"},"user_tz":240},"id":"YbJUnZRVj0Vy"},"outputs":[],"source":["#Load the most recent model\n","def load_latest_checkpoint(checkpoint_dir):\n","  checkpoint_files = sorted(os.listdir(checkpoint_dir), key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x)))\n","  latest_checkpoint = checkpoint_files[-1] if checkpoint_files else None\n","  if latest_checkpoint:\n","    latest_checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n","\n","    model = DiffusionModel(beta_start=0.0001, beta_end=0.02, steps=1000, device=\"cuda\")\n","    model.load_state_dict(torch.load(latest_checkpoint_path))\n","\n","    print(f\"Loaded checkpoint '{latest_checkpoint}'\")\n","    return model\n","  else:\n","      print(\"No checkpoints found.\")\n","      model = DiffusionModel(beta_start=0.0001, beta_end=0.02, steps=1000, device=\"cuda\")\n","      return model"]},{"cell_type":"code","source":["# Setup parameters\n","class Params:\n","    def __init__(self):\n","        self.epochs = 10\n","        self.batch_size = 16\n","        self.learning_rate = 1e-4\n","\n","config = Params()\n","\n","transform = transforms.Compose([\n","    transforms.Resize((512, 512)),\n","    transforms.CenterCrop((512, 512)),\n","    transforms.ToTensor(),\n","])\n","\n","T = 1000  # Total number of diffusion steps\n","\n","# Initialize Weights & Biases\n","#wandb.init(project='my_diffusion_project', entity='michaelpeng72', config=config, id='diffusion_training', resume = 'allow')\n","\n","# Initialize dataset\n","dataset_path = '/content/drive/My Drive/Final Project/datasets'\n","img_dir = os.path.join(dataset_path, 'images')\n","caption_file = os.path.join(dataset_path, 'map.json')\n","full_dataset = ImageCaptionDataset(img_dir, caption_file, transform=transform)\n","\n","train_size = int(0.8 * len(full_dataset))\n","valid_size = len(full_dataset) - train_size\n","train_dataset, valid_dataset = torch.utils.data.random_split(full_dataset, [train_size, valid_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False)\n","\n","# Initialize model\n","model = load_latest_checkpoint('/content/drive/My Drive/Final Project/checkpoints/second_model')\n","optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"lIpbBzmFjow6","executionInfo":{"status":"error","timestamp":1715701681685,"user_tz":240,"elapsed":1259,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"}},"outputId":"accce008-b6db-440e-c232-e6691bba5783"},"execution_count":12,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/My Drive/Final Project/datasets/map.json'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-9e8a8d94fa32>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mcaption_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'map.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageCaptionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ee5477613503>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_dir, caption_file, transform)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Final Project/datasets/map.json'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUpEknK0qMZl","executionInfo":{"status":"aborted","timestamp":1715701187480,"user_tz":240,"elapsed":1,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"}}},"outputs":[],"source":["# Training\n","for epoch in range(0, config.epochs):\n","    print(\"Starting Epoch: \", epoch)\n","    model.train()\n","    train_loss = 0\n","    total_bits = 0\n","    total_pixels = 0\n","\n","    for images, captions in train_loader:\n","        # Model inputs\n","        m_rgb = apply_canny_edge_detector_rgb(images)\n","        m = apply_canny_edge_detector(images)\n","        t = torch.randint(0, T, (1,)).item()\n","\n","        # Forward diffusion process\n","        noised_images = model.forward_pass(m, m_rgb, captions, t)\n","\n","        # Reverse diffusion process\n","        recovered_images = model.reverse_pass(noised_images, m, t)\n","\n","        # Calculate loss\n","        loss = model.diffusion_loss((noised_images - images), (noised_images - recovered_images))\n","        train_loss += loss.item()\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        #wandb.log({\"train_loss_by_batch\": loss.item()})\n","        total_bits += m.sum().item()\n","        total_pixels += images.numel() / images.shape[1]\n","\n","    train_loss /= len(train_loader)\n","    bpp = total_bits / total_pixels\n","    #wandb.log({\"train_loss_by_epoch\": train_loss})\n","\n","    # Validation\n","    model.eval()\n","    valid_loss = 0\n","    with torch.no_grad():\n","        for images, captions in valid_loader:\n","            m_rgb = apply_canny_edge_detector_rgb(images)\n","            m = apply_canny_edge_detector(images)\n","            t = torch.randint(0, T, (1,)).item()\n","\n","            # Same forward and reverse pass for validation\n","            noised_images = model.forward_pass(m, m_rgb, captions, t)\n","            recovered_images = model.reverse_pass(noised_images, m, t)\n","\n","            # Calculate validation loss\n","            loss = model.diffusion_loss((noised_images - images), (noised_images - recovered_images))\n","            valid_loss += loss.item()\n","\n","    valid_loss /= len(valid_loader)\n","    #wandb.log({\"valid_loss\": valid_loss})\n","\n","    compression_rate = np.mean(compression_rate)\n","    #wandb.log({\"compression_rate\": compression_rate})\n","\n","    #Log all outputs\n","    print(\"Epoch: \", epoch, \"Train Loss: \", train_loss, \"Valid Loss: \", valid_loss, \"Compression Rate: \", bpp)\n","\n","    # Save checkpoint\n","    checkpoint_path = f'/content/drive/My Drive/Final Project/checkpoints/epoch_{epoch}_checkpoint.pth'\n","    torch.save(model.state_dict(), checkpoint_path)\n","\n","#wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akIMUcT3qMZl","executionInfo":{"status":"aborted","timestamp":1715701187480,"user_tz":240,"elapsed":377977,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"}}},"outputs":[],"source":["#load model from ../models\n","model = load_latest_checkpoint('/content/drive/My Drive/Final Project/checkpoints')"]},{"cell_type":"markdown","source":["Benchmark Inference Times"],"metadata":{"id":"YVW9LcF5iWro"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wq7qkaCHqMZl","executionInfo":{"status":"aborted","timestamp":1715701187480,"user_tz":240,"elapsed":377976,"user":{"displayName":"Prince Patel","userId":"13151226868257737349"}}},"outputs":[],"source":["# Initialize model\n","model = load_latest_checkpoint('/content/drive/My Drive/Final Project/checkpoints/first_model')\n","config.batch_size = 1\n","# Function to measure inference speed\n","def measure_inference_speed(model, loader, batch_sizes, T):\n","    times = {bs: [] for bs in batch_sizes}\n","\n","    for bs in batch_sizes:\n","        print(bs)\n","        model.eval()\n","        batch_data = []\n","        images_processed = 0\n","        for images, captions in loader:\n","            if len(batch_data) < bs:\n","                batch_data.append((images, captions))\n","            if len(batch_data) >= bs:\n","                images = torch.cat([data[0] for data in batch_data], dim=0)\n","                captions = [caption for data in batch_data for caption in data[1]]\n","                batch_data = []\n","\n","                m_rgb = apply_canny_edge_detector_rgb(images)\n","                m = apply_canny_edge_detector(images)\n","                t = torch.randint(0, T, (1,)).item()\n","\n","                start_time = time.time()\n","\n","                # Forward diffusion process\n","                noised_images = model.forward_pass(m, m_rgb, captions, t)\n","\n","                # Reverse diffusion process\n","                recovered_images = model.reverse_pass(noised_images, m, t)\n","\n","                end_time = time.time()\n","                times[bs].append(end_time - start_time)\n","\n","\n","                break\n","\n","    for bs in batch_sizes:\n","        if times[bs]:  # Check to avoid division by zero\n","            print(f'Average inference time for batch size {bs}: {sum(times[bs]) / len(times[bs]):.4f} seconds')\n","        else:\n","            print(f'No data for batch size {bs}')\n","\n","# Measure inference speed\n","batch_sizes = [1, 5, 25, 50, 100]\n","measure_inference_speed(model, valid_loader, batch_sizes, T)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}